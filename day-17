SERVICES:

SERVICE: It is used to expose the application in k8s.

TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: sv1
spec:
  type: ClusterIP
  selector:
    app: swiggy
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: It will expose our application in a particular port.
Range: 30000 - 32767 (in sg we need to give all traffic)

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: NodePort
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80
      nodePort: 31111

NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC & SSH)
DRAWBACK:
PORT RESTRICTION.

3. LOADBALACER: It will expose our app and distribute load blw pods.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80



AUTO SCALING:
In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or ReplicaSet), with the aim of automatically scaling the workload to match demand.
Vertical means Existing
Horizontal means New

Example : if you have pod-1 with 40% load and pod2 with 40% load then average will be (40+40/2=40) average value is 40
but if pod-1 is exceeding 50% and pod-2 40% then average will be 45% (then here we need to create a pod-3 becaue its exceeding the average)

Here we need to use metric server whose work is to collect the metrics (cpu & mem info)
metrics server is connected to the HPA and give information to HPA 
Now HPA will analysis metrics for every 30 sec and create a new pod if needed.


COOLING PERIOD:


scaling can be done only for scalabel objects (ex: RS, Deployment, RC )
HPA is implemented as a K8S API Resources and a controller.
Controller Periodically adjust the number of replicas in RS, RC and Deployment depends on average.

HPA:


For resource metrics (like CPU) the Controller fetches the metrics from the resource metrics All for each pod targeted by the HPA.

Then if a target utilization Value is Set, the Controller calculates the Utilization value as a percentage of the Equivalent resource request on the Containers in each pod.

If a target raw value is set, raw metric values are used direct! then The Controler takes the average of utilization.

The cooldown period to wait before another downscale Operation Can be performed is controlled by
- horizontal -pod-autoscaler-downtime- stabilization flag (Default value is 5 Min)

--kubelete-insecure-tls

kubectl autoscale deployment mydeploy --cpu-percent=20 --min=1 --max=10


copy the content on componets.yml on my local
edit line number: 137 --- > - --kubelete-insecure-tls

kubectl apply -f file

(or)

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/components.yaml


kubectl get ns
kubectl get po -n kube-system
now you will see the metrics server
kubectl get logs -f metrics-server-7fcb88f75-lm7hp -n kube-system


vim hpa.yml

kind: Deployment
apiVersion: apps/v1
metadata:
   name: raham
spec:
   replicas: 1
   selector:
    matchLabels:
     name: app
   template:
     metadata:
       name: testpod8
       labels:
         name: app
     spec:
      containers:
        - name: c00
          image: httpd
          ports:
          - containerPort: 80
          resources:
            limits:
              cpu: 500m
            requests:
              cpu: 200m

kubectl apply -f hpa.yml
kubectl get all
kubectl get deploy 
kubectl autoscale deployment raham --cpu-percent=20 --min=1 --max=10
kubectl get al1

now opne second terminal 
kubectl get po
kubectl exec pod_name_id -it -- /bin/bash (this will not take you inside )

go to terminal one 
kubectl get all : it will observer for every 2 seconds

now opne second terminal
apt update -y
apt upgrade -y


Now it will create extra pods in the terminal one

now if the load gets down then after 5 minutes all the pods will go
these 5 minutes is called as cooldown period 


